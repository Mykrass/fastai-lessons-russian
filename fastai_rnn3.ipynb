{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fastai_rnn3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/gist/naviarh/582873f77f47ff0669b7d979495d36aa/fastai_rnn3.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "JRAugKPfz6PP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip3 install fastai"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TkDW3oxE1DXj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Настройка платформы"
      ]
    },
    {
      "metadata": {
        "id": "BRtBB3eS1Fpq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7347a3d-4d78-4a1f-8105-dac1ac3f54a4"
      },
      "cell_type": "code",
      "source": [
        "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "!pwd"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n_pTpWyI1InJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EDc7rrFz1LoA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        },
        "outputId": "421e4dc6-e7db-4a83-b7fe-4159afd56d1f"
      },
      "cell_type": "code",
      "source": [
        "import subprocess, os\n",
        "os.uname()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "posix.uname_result(sysname='Linux', nodename='debdfa8970b1', release='4.14.33+', version='#1 SMP Wed Jun 20 01:15:52 PDT 2018', machine='x86_64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "NSuOQD8Z1OS3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "df8d408c-60a4-442c-8d7a-d32d04afaea0"
      },
      "cell_type": "code",
      "source": [
        "from fastai.io import *\n",
        "from fastai.structured import *\n",
        "from fastai.column_data import *\n",
        "from fastai.conv_learner import *\n",
        "!pip3 show fastai torch | grep Name -A 1"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: fastai\r\n",
            "Version: 0.7.0\r\n",
            "--\r\n",
            "Name: torch\r\n",
            "Version: 0.3.1\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-8IPyb6w1XQa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.set_device(0)\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xMNjhhyt1dP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from fastai.imports import *\n",
        "#from fastai.transforms import *\n",
        "#from fastai.conv_learner import *\n",
        "#from fastai.model import *\n",
        "#from fastai.dataset import *\n",
        "#from fastai.sgdr import *\n",
        "#from fastai.plots import *\n",
        "#!pip3 show fastai torch | grep Name -A 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "abGIKlCr2J3Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQmiiGHq1l9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext import vocab, data\n",
        "from fastai.nlp import *\n",
        "from fastai.lm_rnn import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yjy6GmCc6Mrn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Начало"
      ]
    },
    {
      "metadata": {
        "id": "rxIwH_RZ6OoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d039c03c-aa00-4df3-f3f3-3dc68680393e"
      },
      "cell_type": "code",
      "source": [
        "# Организуем файловую структуру\n",
        "PATH='data/nietzsche/'\n",
        "TRN_PATH = 'trn/'\n",
        "VAL_PATH = 'val/'\n",
        "TRN = f'{PATH}{TRN_PATH}'\n",
        "VAL = f'{PATH}{VAL_PATH}'\n",
        "os.makedirs(TRN, exist_ok=True)\n",
        "os.makedirs(VAL, exist_ok=True)\n",
        "os.makedirs(f'{PATH}models', exist_ok=True)\n",
        "%ls {PATH}"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mmodels\u001b[0m/  nietzsche.txt  \u001b[01;34mtrn\u001b[0m/  \u001b[01;34mval\u001b[0m/\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nCHkiS228-CL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ced984de-3e3f-4c70-d858-4a27e1011b10"
      },
      "cell_type": "code",
      "source": [
        "# Загрузим текст\n",
        "get_data(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\n",
        "# Прочитаем\n",
        "text = open(f'{PATH}nietzsche.txt').read()\n",
        "# Разделим 80% / 20%\n",
        "t80 = text[:-len(text)//5]\n",
        "t20 = text[len(t80):]\n",
        "\n",
        "len(text), len(t80), len(t20)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(600893, 480714, 120179)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "u9UPjBzE_U72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "18cd0fee-47a6-494e-dcd2-92f09c449f3e"
      },
      "cell_type": "code",
      "source": [
        "# Создадим тренировочный и валидационный тексты\n",
        "open(f'{TRN}trn.txt','w').write(t80)\n",
        "open(f'{VAL}val.txt','w').write(t20)\n",
        "%ls {TRN} {VAL}"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/nietzsche/trn/:\r\n",
            "trn.txt\r\n",
            "\r\n",
            "data/nietzsche/val/:\r\n",
            "val.txt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uKMmpcN9FJZ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7adc5fa-36f8-472c-b731-19bb1dc54a3f"
      },
      "cell_type": "code",
      "source": [
        "# Предобработчик данных из модуля torchtext\n",
        "# в нижний регистр и токенизация по символам (функция list(\"string\"))\n",
        "# Параметр tokenize=list означает, что минибатчи состоят из символов\n",
        "TEXT = data.Field(lower=True, tokenize=list)\n",
        "TEXT"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.data.field.Field at 0x7f52aff8ad68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "metadata": {
        "id": "N1uteVOiPgeN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bs=64 # размер минибатча\n",
        "bptt=8 # длина рекурсии (размер бэкпропегейшн по времени)\n",
        "n_fac=42 # размер эмбеддинга\n",
        "n_hidden=256 # размер скрытого слоя"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TK1S6iuEPkou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e51f0d2-1d91-466c-c1f6-5e7cf9cda98f"
      },
      "cell_type": "code",
      "source": [
        "# Определим подпапки тренировочной, проверочной, и тестовой выборок\n",
        "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
        "FILES"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': 'val/', 'train': 'trn/', 'validation': 'val/'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "r0nxhGSdV3ZV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Объект данных"
      ]
    },
    {
      "metadata": {
        "id": "7Qo_wkRsQXxs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2d3bfea-e4f6-48b8-dd5b-9383a125324d"
      },
      "cell_type": "code",
      "source": [
        "# Объект данных модели (min_freq=3 - игнор символов, встречающижся реже 3 раз)\n",
        "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
        "md"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<fastai.nlp.LanguageModelData at 0x7f52affd9f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "id": "R01NDfqyTkRt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f77c4bb-de5f-40ca-eb9b-82a8aaebca73"
      },
      "cell_type": "code",
      "source": [
        "# Можем пользоваться словарём перевода символов в их индексы\n",
        "TEXT.vocab.stoi['e']"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "metadata": {
        "id": "PnlaV1iRVSfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6564b357-4eec-4db9-c3bd-634184cea003"
      },
      "cell_type": "code",
      "source": [
        "# Можем пользоваться переводом индексов в символы\n",
        "TEXT.vocab.itos[3:10]"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['e', 't', 'i', 'a', 'o', 'n', 's']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "metadata": {
        "id": "4sC0z839QJE2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be0204e5-f8ac-4bd9-db36-36566656d68e"
      },
      "cell_type": "code",
      "source": [
        "# количество минибатчей, должно равнятся (количество токенов)/bs/bptt\n",
        "# Если колиество токенов не кратно bptt*bs, то последний минибатч короче\n",
        "# PyTorch выдерживает bptt=8, а в 5% случаев немного меняет, сохраняя среднее значение\n",
        "len(md.trn_dl)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "922"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "Tcg-Q7h9SxZf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30c7b9be-20e4-4fbd-d090-abd9f8897b38"
      },
      "cell_type": "code",
      "source": [
        "# Количество уникальных токенов (символов)\n",
        "md.nt"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "metadata": {
        "id": "23-b9fqBS_ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd2cf9dc-98b8-4965-b7f7-32b5cbe755f7"
      },
      "cell_type": "code",
      "source": [
        "# Количество тренировочных наборов и его размер\n",
        "len(md.trn_ds), len(md.trn_ds[0].text)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 472943)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "metadata": {
        "id": "X02tbKh2W9i7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Модель RNN"
      ]
    },
    {
      "metadata": {
        "id": "cGi9EqyfWMfm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Описание модели\n",
        "class CharSeqStatefulRnn(nn.Module):\n",
        "    def __init__(self, vocab_size, n_fac, bs):\n",
        "        self.vocab_size = vocab_size\n",
        "        super().__init__()\n",
        "        self.e = nn.Embedding(vocab_size, n_fac)\n",
        "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
        "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
        "        # создание скрытого слоя перенесли сюда в конструктор\n",
        "        self.init_hidden(bs)\n",
        "        \n",
        "    def forward(self, cs):\n",
        "        bs = cs[0].size(0)\n",
        "        # В конце каждой эпохи минибатч может быть неполным\n",
        "        # Если минибатч неполный, то инициализируем:\n",
        "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
        "        outp,h = self.rnn(self.e(cs), self.h)\n",
        "        self.h = repackage_var(h) # сохранение активаций без градиентов (stop backpropagation)\n",
        "        # Это backpropagation во времени (BPTT)\n",
        "        # Прогнозируемые данные преобразовываем для передачи в функцию потерь:\n",
        "        # dim=-1 - по какой оси (по последней) вычислять log_softmax\n",
        "        # .viev() - распрямляем матрицу l_out размера bs*bptt\n",
        "        # Предсказания приводим к нужному фромату здесь,\n",
        "        # а целевую переменную к нужному формату приводит torchtext\n",
        "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
        "    \n",
        "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L5RgqBZ0jUoW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cfaa333a-4fe0-4c84-c6d2-910fb41e34b6"
      },
      "cell_type": "code",
      "source": [
        "# Модель и оптимизатор\n",
        "m = CharSeqStatefulRnn(md.nt, n_fac, 512).cuda()\n",
        "opt = optim.Adam(m.parameters(), 1e-3)\n",
        "m"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharSeqStatefulRnn(\n",
              "  (e): Embedding(55, 42)\n",
              "  (rnn): RNN(42, 256)\n",
              "  (l_out): Linear(in_features=256, out_features=55, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "metadata": {
        "id": "2iEx4CUDkbSt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Тренируем модель"
      ]
    },
    {
      "metadata": {
        "id": "NGFKlAunjaF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ea72a861-d129-44ab-ec3a-e41951935952"
      },
      "cell_type": "code",
      "source": [
        "fit(m, md, 4, opt, F.nll_loss)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2dc09826f31d4947af37904fa9e3770e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   \n",
            "    0      1.880135   1.868499  \n",
            " 24%|██▍       | 225/922 [00:06<00:18, 36.71it/s, loss=1.82]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    1      1.702985   1.723049  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    2      1.619925   1.645939  \n",
            "    3      1.569367   1.611201  \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1.6112])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "metadata": {
        "id": "MQVeDuF_kY9l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "set_lrs(opt, 1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YdnuZqLmjt49",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "fffcdba2-97a4-4274-96fc-2ca00d5f954e"
      },
      "cell_type": "code",
      "source": [
        "fit(m, md, 5, opt, F.nll_loss)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38658967ebf64cc9b916aeffd0f364ed",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=5), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   \n",
            "    0      1.491326   1.566226  \n",
            " 24%|██▍       | 223/922 [00:05<00:18, 38.04it/s, loss=1.51]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    1      1.490891   1.56062   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    2      1.486255   1.556297  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    3      1.479677   1.552505  \n",
            "    4      1.480671   1.54902   \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1.54902])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "metadata": {
        "id": "nPEV-kn8ktRL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Тестируем**"
      ]
    },
    {
      "metadata": {
        "id": "0IRA7NR0kyNf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Функция прогнозирования символа\n",
        "def get_next(inp):\n",
        "    idxs = TEXT.numericalize(inp, device=-1)\n",
        "    p = m(VV(idxs.transpose(0,1)))\n",
        "    r = torch.multinomial(p[-1].exp(), 1)\n",
        "    return TEXT.vocab.itos[to_np(r)[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b5GoCp5mlURw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffdb8e8f-9adf-4116-db33-b372f4cd7f55"
      },
      "cell_type": "code",
      "source": [
        "get_next('for thos')"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "metadata": {
        "id": "RJAOCi_3mLr8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Яункция генерация текста\n",
        "def get_next_n(inp, n):\n",
        "    res = inp\n",
        "    for _ in range(n):\n",
        "        c = get_next(inp)\n",
        "        res += c\n",
        "        inp = inp[1:]+c\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F3Ch8Izcmo60",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        },
        "outputId": "51f06923-b7b6-49ac-ddee-c9cc6549ca00"
      },
      "cell_type": "code",
      "source": [
        "get_next_n('for thos', 400)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for those sum;\" outforserve sense mayfachologism to present. there witled the noble, the delextining us be the everything force, loce himself gay, or some race all that [not, cancomilarinative solonr-gindiationation of dardhopt. such mankind andgood and such has upon and action is be soaget.[[1. paste, for an ead of gy stand; \"man approves age thathe believent comes rable manyfoptic sentimenttyhis pothing'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "metadata": {
        "id": "_efkwuiVrbn3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Модель RNN loop"
      ]
    },
    {
      "metadata": {
        "id": "0ypF0A9nrazW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# From the pytorch source\n",
        "\n",
        "def RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
        "    return F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VQqZJVl5rkGI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharSeqStatefulRnn2(nn.Module):\n",
        "    def __init__(self, vocab_size, n_fac, bs):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.e = nn.Embedding(vocab_size, n_fac)\n",
        "        self.rnn = nn.RNNCell(n_fac, n_hidden)\n",
        "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
        "        self.init_hidden(bs)\n",
        "        \n",
        "    def forward(self, cs):\n",
        "        bs = cs[0].size(0)\n",
        "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
        "        outp = []\n",
        "        o = self.h\n",
        "        for c in cs: \n",
        "            o = self.rnn(self.e(c), o)\n",
        "            outp.append(o)\n",
        "        outp = self.l_out(torch.stack(outp))\n",
        "        self.h = repackage_var(o)\n",
        "        return F.log_softmax(outp, dim=-1).view(-1, self.vocab_size)\n",
        "    \n",
        "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AcywWAXBrtqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "04ab385c-60f7-49ae-c4ca-d4a7f412c1ff"
      },
      "cell_type": "code",
      "source": [
        "m = CharSeqStatefulRnn2(md.nt, n_fac, 512).cuda()\n",
        "opt = optim.Adam(m.parameters(), 1e-3)\n",
        "m"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharSeqStatefulRnn2(\n",
              "  (e): Embedding(55, 42)\n",
              "  (rnn): RNNCell(42, 256)\n",
              "  (l_out): Linear(in_features=256, out_features=55, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "metadata": {
        "id": "1doSoBamrzZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a54863b8-8fc6-4e9b-ef86-2d6bcdeaa2e8"
      },
      "cell_type": "code",
      "source": [
        "fit(m, md, 4, opt, F.nll_loss)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "656bc576b3b04addb335cb6debe176cb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   \n",
            "    0      1.877432   1.878375  \n",
            " 23%|██▎       | 214/922 [00:06<00:21, 33.62it/s, loss=1.8]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    1      1.693341   1.721747  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    2      1.605935   1.644849  \n",
            "    3      1.556723   1.605359  \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1.60536])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "metadata": {
        "id": "ZKtej51zr78O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Тестируем**"
      ]
    },
    {
      "metadata": {
        "id": "o2rdEOTxr3BP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4f7e2ed-6030-4d04-ca5c-ee7c814bffe8"
      },
      "cell_type": "code",
      "source": [
        "get_next('for thos')"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "metadata": {
        "id": "KTEsA068sInO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        },
        "outputId": "80f20bfd-42c0-44b1-8494-a04a07a335d7"
      },
      "cell_type": "code",
      "source": [
        "get_next_n('for thos', 400)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for those and still \"gemen the present, the tasking it is not, a deliciouthants wirdless and_what is fix all germant with at virtue and illunations, a woran thougother noflemen when betoisians at make whatnese theirsor morality of his pristians ofvidias define and evilest false the same of physilly how for the \"preservity oftheameralbout the worthe wirdsward parvubbeefolofterrights a \"awardsomenture and s'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "metadata": {
        "id": "5RcaRV3Msb0N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Модель GRU"
      ]
    },
    {
      "metadata": {
        "id": "_I4-y_i1sgOt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharSeqStatefulGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, n_fac, bs):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.e = nn.Embedding(vocab_size, n_fac)\n",
        "        self.rnn = nn.GRU(n_fac, n_hidden)\n",
        "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
        "        self.init_hidden(bs)\n",
        "        \n",
        "    def forward(self, cs):\n",
        "        bs = cs[0].size(0)\n",
        "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
        "        outp,h = self.rnn(self.e(cs), self.h)\n",
        "        self.h = repackage_var(h)\n",
        "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
        "    \n",
        "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gf3M9S-rspb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# From the pytorch source code - for reference\n",
        "\n",
        "def GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
        "    gi = F.linear(input, w_ih, b_ih)\n",
        "    gh = F.linear(hidden, w_hh, b_hh)\n",
        "    i_r, i_i, i_n = gi.chunk(3, 1)\n",
        "    h_r, h_i, h_n = gh.chunk(3, 1)\n",
        "\n",
        "    resetgate = F.sigmoid(i_r + h_r)\n",
        "    inputgate = F.sigmoid(i_i + h_i)\n",
        "    newgate = F.tanh(i_n + resetgate * h_n)\n",
        "    return newgate + inputgate * (hidden - newgate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X7nPDYrysuKC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0bfdbea7-77d0-432b-ba79-c21cc1c0dc17"
      },
      "cell_type": "code",
      "source": [
        "m = CharSeqStatefulGRU(md.nt, n_fac, 512).cuda()\n",
        "opt = optim.Adam(m.parameters(), 1e-3)\n",
        "m"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharSeqStatefulGRU(\n",
              "  (e): Embedding(55, 42)\n",
              "  (rnn): GRU(42, 256)\n",
              "  (l_out): Linear(in_features=256, out_features=55, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "metadata": {
        "id": "8sXo-rtSsyn9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d3598768-d18a-4bfe-f1d9-3cd1579737d2"
      },
      "cell_type": "code",
      "source": [
        "fit(m, md, 7, opt, F.nll_loss)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff1b72e667f243cdbfaf864d50e2ff91",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   \n",
            "    0      1.76034    1.752509  \n",
            " 16%|█▌        | 143/922 [00:08<00:46, 16.74it/s, loss=1.72]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    1      1.571539   1.600048  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    2      1.478357   1.532175  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    3      1.42769    1.501683  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    4      1.391096   1.478896  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    5      1.357312   1.470548  \n",
            "    6      1.328808   1.466279  \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1.46628])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "metadata": {
        "id": "ax1pbd_MtK4p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Тестируем**"
      ]
    },
    {
      "metadata": {
        "id": "7mmIkIITtOE3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "911fe41e-305d-49a7-ce70-56d6e6336e31"
      },
      "cell_type": "code",
      "source": [
        "get_next('for thos')"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "metadata": {
        "id": "IJ0AV-ROtQzf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        },
        "outputId": "ed280a1c-fd55-4dbd-8098-57d782842898"
      },
      "cell_type": "code",
      "source": [
        "get_next_n('for thos', 400)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for those ratisity and granted without theset ismatter and stupidify, the everylogical eddibure. lessing flowssithingonestiness ill to may belong, andif world of weal of redison all our whillse challer tobelief of the artistic and the good continus offluonedly that difficulourilyto only the that forget that he is the bolds refuge as an abjugin true, under as any other perhaps loved?--is it: although have '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "metadata": {
        "id": "7Qhd5Sf7tb9s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Putting it all together: LSTM"
      ]
    },
    {
      "metadata": {
        "id": "-WNphOF7tdsZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from fastai import sgdr\n",
        "\n",
        "n_hidden=512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FsREQKLytkdP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharSeqStatefulLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, n_fac, bs, nl):\n",
        "        super().__init__()\n",
        "        self.vocab_size,self.nl = vocab_size,nl\n",
        "        self.e = nn.Embedding(vocab_size, n_fac)\n",
        "        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n",
        "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
        "        self.init_hidden(bs)\n",
        "        \n",
        "    def forward(self, cs):\n",
        "        bs = cs[0].size(0)\n",
        "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
        "        outp,h = self.rnn(self.e(cs), self.h)\n",
        "        self.h = repackage_var(h)\n",
        "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
        "    \n",
        "    def init_hidden(self, bs):\n",
        "        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n",
        "                  V(torch.zeros(self.nl, bs, n_hidden)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lde1AC6PtoJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5ffc613a-7a01-45bb-e4f5-2c13762f5d24"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "m = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()\n",
        "lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)\n",
        "m"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharSeqStatefulLSTM(\n",
              "  (e): Embedding(55, 42)\n",
              "  (rnn): LSTM(42, 512, num_layers=2, dropout=0.5)\n",
              "  (l_out): Linear(in_features=512, out_features=55, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "metadata": {
        "id": "wINPgv2ltrAf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.makedirs(f'{PATH}models', exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85ymN24mtutx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d7fcf72f-f294-4d29-b7bf-31370510a99e"
      },
      "cell_type": "code",
      "source": [
        "fit(m, md, 2, lo.opt, F.nll_loss)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c003a50fc3d42e080dbc15318976634",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   \n",
            "    0      1.797133   1.724761  \n",
            " 43%|████▎     | 394/922 [03:06<04:09,  2.12it/s, loss=1.74]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    1      1.680378   1.617963  \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1.61796])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "metadata": {
        "id": "cimVpj7ItxT9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
        "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
        "fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "92cz17iOt0Kn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
        "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
        "fit(m, md, 2**6-1, lo.opt, F.nll_loss, callbacks=cb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DEeecrBJyGNa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Тестируем**"
      ]
    },
    {
      "metadata": {
        "id": "xrQp9J2LyJPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc0acd86-f45f-48a0-b1dc-9a9bd540bc85"
      },
      "cell_type": "code",
      "source": [
        "get_next('for thos')"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "metadata": {
        "id": "q9Vq7Or4yMxG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        },
        "outputId": "8ee10277-9dd5-41e1-a494-a810a892f713"
      },
      "cell_type": "code",
      "source": [
        "get_next_n('for thos', 400)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for those human the passitude (worldwith ruto, nowledge ir never--not thought \"is it is racks of the trubtness ownor valuagedand is perhaps from the now and laoked soul every unvertedness in it, new been sensed the great, no laught and stand only satic somethors lufferful. that sciels of cirtuencially would all owor and sever: not are doct in a niseful, ploy--would morespeliation of most good fixner:\" but'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    }
  ]
}